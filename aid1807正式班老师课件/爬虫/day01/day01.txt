王伟超
  wangweichao@tedu.cn
1、网络爬虫
  1、定义 ：网络蜘蛛、网络机器人,抓取网络数据的程序
  2、总结 ：用Python程序模仿人去访问网站,模仿的越像人越好
  3、爬取数据的目的 ：通过有效的大量数据分析市场走势、公司决策
2、企业获取数据的方式
  1、公司自有数据
  2、第三方数据平台购买
    数据堂、贵阳大数据交易所
  3、爬虫爬取数据
    市场上没有或者价格太高,利用爬虫程序爬取
3、Python做爬虫优势
  请求模块、解析模块丰富成熟,强大的scrapy框架
  PHP ：对多线程、异步支持不太好
  JAVA ：代码笨重,代码量很大
  C/C++：虽然效率高,但是代码成型很慢
4、爬虫分类
  1、通用网络爬虫(搜索引擎引用,需要遵守robots协议)
    http://www.taobao.com/robots.txt
    1、搜索引擎如何获取一个新网站的URL
      1、网站主动向搜索引擎提供(百度站长平台)
      2、和DNS服务商(万网)合作,快速收录新网站
  2、聚焦网络爬虫
    自己写的爬虫程序 ：面向需求的爬虫
5、爬取数据的步骤
  1、确定要爬取的URL地址
  2、通过HTTP/HTTPS协议获取相应的HTML页面
  3、提取HTML页面中有用的数据
    1、所需数据,保存
    2、页面中有其他的URL,继续第2步
6、Anaconda 和 Spyder
  1、Spyder常用快捷键
    1、注释/取消注释 ：Ctrl + 1
    2、保存 ：ctrl + s
    3、运行 ：F5
    4、自动补全 ：Tab
7、Chrome浏览器插件
  1、安装步骤
    1、浏览器右上角 -> 更多工具 -> 扩展程序
    2、点开右上角 -> 开发者模式
    3、拖拽 插件 到浏览器页面,释放鼠标 -> 添加扩展程序
  2、插件介绍
    1、Xpath Helper : 网页数据解析插件
    2、JSON View    : 查看json格式的数据(好看)
    3、Proxy SwitchOmega : 代理切换插件
8、WEB
  1、HTTP和HTTPS
    1、HTTP : 80
    2、HTTPS : 443,HTTP的升级版,加了一个安全套接层
  2、GET和POST
    1、GET ：查询参数都会在URL地址上显示出来
    2、POST ：查询参数和需要提交数据是隐藏在form表单里的,不会在URL上显示
  3、URL ：统一资源定位符
    https: //item.jd.com :80/443 /11936238.html #detail
    协议   域名/IP地址    端口  访问资源的路径  锚点
  4、User-Agent
    记录用户的浏览器、操作系统等,为了让用户获取更好的HTML页面效果
    User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_7_0) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11

    Mozilla Firefox : (Gecko内核)
    IE  ：Trident(自己的内核)
    Linux : KHTML(like Gecko)
    Apple : Webkit(like KHTML)
    Google : Chrome(like Webkit)
    其他浏览器都是模仿IE/Chrome
9、爬虫请求模块
  1、版本
    1、python2 ：urllib2  urllib
    2、python3 ：urllib.request
  2、常用方法(urllib.request)
    1、urllib.request.urlopen("网址")
      1、作用 ：向网站发起请求,并获取响应
        字节流 = response.read()
	字符串 = response.read().decode("utf-8")
	# encode() 字符串-->bytes
	# decode() bytes -->字符串
      2、重构User-Agent
        1、不支持重构 ：urlopen()
	2、支持重构User-Agent
	  urllib.request.Request(添加User-Agent)
    2、urllib.request.Request(url,headers={"User..})
      User-Agent是爬虫和反爬虫斗争的第1步,发送请求必须带User-Agent
      1、使用流程
        1、创建请求对象 ：利用Request()方法
	2、获取响应对象 ：利用urlopen(请求对象)
	3、获取响应内容 ：html=res.read().decode("utf-8")
      2、响应对象res的方法
        1、read() : 读取响应内容(字节流)
	2、getcode() : 返回服务器响应码
	  print(res.getcode())
	  200 : 成功
	  4XX : 服务器页面出错
	  5XX : 服务器出错
	3、geturl() : 返回实际数据的URL(防止重定向问题)
  3、urllib.parse模块
    1、urllib.parse.urlencode({字典})
      编码前 ：{"wd":"帅哥"}
      编码后 ："wd=%e8%a5"
      注意
        1、urlencode的参数一定要为 字典,得到的结果为 字符串
	2、with open("Baidu.html","w",encoding="utf-8") as f:
	    f.write(html)
    2、urllib.parse.quote("字符串")
      1、http://www.baidu.com/s?wd=帅哥
      2、key = urllib.parse.quote("帅哥")
         key的值 ："%e8%a3%b4..."
    3、urllib.parse.unquote("字符串")
      s = "%d3%e8..."
      s = urllib.parse.unquote(s)
  4、练习
    百度贴吧数据抓取
    要求：
      1、输入要抓取的贴吧名称：
      2、输入起始页和终止页
      3、把一页保存到本地
        第1页.html 第2页.html ... ... 
    步骤：
      1、找URL规律
        第1页：http://tieba.baidu.com/f?kw=??&pn=0
	第2页：http://tieba.baidu.com/f?kw=??&pn=50
	... ... 
	第n页：pn=(n-1)*50
      2、获取网页内容(发请求,获响应)
      3、保存(本地,数据库)
10、请求方式及案例
  1、GET
    1、特点 ：查询参数在URL地址上显示
    2、案例 ：抓取百度贴吧
  2、POST(在Request方法中添加data参数)
    1、req = urllib.request.Request(url,data=data,headers=headers)
      data ：表单数据以bytes类型提交,不能是string
    2、案例 ：有道翻译
      1、要求
        用户输入需要翻译的内容,把翻译的结果给输出来
    3、如何把json格式的字符串转换为Python中字典
      import json
      s = '{"key":"value"}'
      s_dict = json.loads(s)














